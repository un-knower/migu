user.name=hadoop-user
es.nodes=10.0.30.106,10.0.30.107
#es.nodes=103.78.126.43
#spark.sql.warehouse.dir=hdfs://103.78.126.39:9000/user/hive/warehouse
spark.sql.warehouse.dir=hdfs://10.0.30.101:9000/user/hive/warehouse
#es.nodes=10.0.10.4,10.0.10.5
es.port=9200

#kafka.brokers=10.0.30.103:9092,10.0.30.104:9092
#kafka.topics=cachetest
kafka.consumer=null
auto.offset.reset=smallest
#zookeeper.nodes=null
day.executor.memory=5g
hour.executor.memory=5g
day.cores.max=10
hour.cores.max=3

#mysqlConn=jdbc:mysql://103.78.126.39:3306/rmp?useUnicode=true&characterEncoding=utf-8
#mysqlConn=jdbc:mysql://10.0.10.3:3306/rmp?useUnicode=true&characterEncoding=utf-8
mysqlConn=jdbc:mysql://10.0.30.14:3306/rmp?useUnicode=true&characterEncoding=utf-8
mysql.user=root
#开发环境
mysql.pwd=qwertyu
#mysql.pwd=123321


#flume.test.big.node=10.0.30.106
#flume.test.small.node=10.0.30.107
flume.local.big.source=10.0.10.11
flume.local.small.source=10.0.10.12
#flume.local.big.port=20000
#flume.local.small.port=19999

#spark.master=spark://10.0.10.8:7077
spark.master=yarn
#spark.master=spark://10.0.30.101:7077
#spark.master=local[2]
runtime.cores.max=5
queue=scheduler

# 配置hadoop的主机名、端口、hive数据库名
hdfs.host=10.0.30.101
hdfs.port=8020
hdfs.hive.dbname=ifp_source
hdfs.smallfile.path=/ifp_source/DataCenter/SA/TB_SA_Cache_small/
hdfs.bigfile.path=/ifp_source/DataCenter/SA/TB_SA_Cache_big/
hdfs.small.streaming.path=hdfs://10.0.30.101:8020/user/hadoop-user/coll_data/cache_log/sch/280/small/
hdfs.big.streaming.path=hdfs://10.0.30.101:8020/user/hadoop-user/coll_data/cache_log/sch/280/big/


# dw批处理脚本分配的硬件资源
spark.executor.instances=4
spark.executor.cores=2
spark.cores.max=8
spark.executor.memory=16G
small.checkpointDirectory=hdfs://103.78.126.39:8020/Checkpoint_Data_Small
big.checkpointDirectory=hdfs://103.78.126.39:8020/Checkpoint_Data_big
